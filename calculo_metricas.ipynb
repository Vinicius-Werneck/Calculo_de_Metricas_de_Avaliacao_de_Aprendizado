{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plano do Projeto - Classificação de Dígitos MNIST com Redes Neurais e Cálculo de Métricas de Avaliação de Aprendizado.\n",
    "\n",
    "1. Definição do problema: Criar uma rede neural para classifcar imagens de dígitos manuscritos (0 a 9) do dataset MNIST e avaliar o desempenho do modelo com sensibilidade, especificidade, precisão, acurácia e F-score.\n",
    "\n",
    "2. Pré -processamento dos dados: \n",
    "* carregar o dataset MNIST;\n",
    "* Normalizar os valores dos pixels (escala 0 a 1);\n",
    "* Converter os rótulos em one-hot encoding (para usar na classificação com Softmax);\n",
    "* Dividir os dados em treino (80%) e (20%) teste.\n",
    "\n",
    "3. Construção da rede Neural:\n",
    "* Entrada: 28x28 pixels (convertidos em um vetor de 784 elementos);\n",
    "* Camadas ocultas: 2 camadas densas (fully connected) com ativação ReLU;\n",
    "* Saída: 10 neurônios (um para cada dígito) com ativação Sofmax;\n",
    "* Função perda: Categorical Crossentropy;\n",
    "* Otimizador: Adam.\n",
    "\n",
    "4. Treinamento do Modelo:\n",
    "* Utilizar batch size de 32 e treinar por 10 épocas;\n",
    "* Acompanhar acurácia e perda no conjunto de validação.\n",
    "\n",
    "5. Avaliação com as Métricas do Desafio:\n",
    "* Fazer previsões no conjunto de teste;\n",
    "* Construir a matriz de confusão (comparando rótulos reais e previstos);\n",
    "* Calcular sensibilidade, especificidade, precisão, acurácia e F-score para cada classe.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importação das bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. * Carregar e Pré-processar os dados;\n",
    "   * Normalização (deixar os valores dos pixels entre 0 e 1);\n",
    "   * One-hot encoding para os módulos;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalização e reformatação\n",
    "x_train = x_train.reshape(-1, 28*28) / 255.0\n",
    "x_test = x_test.reshape(-1,28*28) / 255.0\n",
    "\n",
    "# One-hot encoding para os rótulos\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test,10) # Guardamos y_test original para a matriz de confusão\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Criar a Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(784,)),  # Define a entrada corretamente\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar o Modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Treinar o Modelo e fazer previsões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8750 - loss: 0.4285 - val_accuracy: 0.9617 - val_loss: 0.1245\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9695 - loss: 0.1030 - val_accuracy: 0.9737 - val_loss: 0.0865\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9784 - loss: 0.0685 - val_accuracy: 0.9739 - val_loss: 0.0868\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9829 - loss: 0.0518 - val_accuracy: 0.9762 - val_loss: 0.0847\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9878 - loss: 0.0378 - val_accuracy: 0.9736 - val_loss: 0.0847\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9895 - loss: 0.0322 - val_accuracy: 0.9779 - val_loss: 0.0806\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9918 - loss: 0.0246 - val_accuracy: 0.9784 - val_loss: 0.0760\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9925 - loss: 0.0207 - val_accuracy: 0.9799 - val_loss: 0.0778\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9926 - loss: 0.0207 - val_accuracy: 0.9775 - val_loss: 0.0900\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9952 - loss: 0.0150 - val_accuracy: 0.9795 - val_loss: 0.0901\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train_cat, epochs=10, batch_size=32, validation_data=(x_test, y_test_cat))\n",
    "\n",
    "#previsões\n",
    "y_pred_prob = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1) # Converte probabilidades em rótulos inteiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculo da matriz confusão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Cálculo das métricas para cada classe (0 a 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_metricas(cm):\n",
    "    metricas = {}\n",
    "    for i in range(10):  # Para cada classe (0 a 9)\n",
    "        VP = cm[i, i]\n",
    "        FN = cm[i, :].sum() - VP\n",
    "        FP = cm[:, i].sum() - VP\n",
    "        VN = cm.sum() - (VP + FP + FN)\n",
    "\n",
    "        sensibilidade = VP / (VP + FN) if (VP + FN) > 0 else 0\n",
    "        especificidade = VN / (VN + FP) if (VN + FP) > 0 else 0\n",
    "        acurácia = (VP + VN) / cm.sum() if cm.sum() > 0 else 0\n",
    "        precisao = VP / (VP + FP) if (VP + FP) > 0 else 0\n",
    "        f_score = 2 * (precisao * sensibilidade) / (precisao + sensibilidade) if (precisao + sensibilidade) > 0 else 0\n",
    "\n",
    "        metricas[f'Dígito {i}'] = [sensibilidade, especificidade, acurácia, precisao, f_score]\n",
    "    \n",
    "    return metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Sensibilidade  Especificidade  Acurácia  Precisão   F-score\n",
      "Dígito 0       0.985714        0.998448    0.9972  0.985714  0.985714\n",
      "Dígito 1       0.987665        0.999098    0.9978  0.992914  0.990283\n",
      "Dígito 2       0.977713        0.997324    0.9953  0.976767  0.977240\n",
      "Dígito 3       0.976238        0.997998    0.9958  0.982072  0.979146\n",
      "Dígito 4       0.982688        0.995564    0.9943  0.960199  0.971314\n",
      "Dígito 5       0.973094        0.998682    0.9964  0.986364  0.979684\n",
      "Dígito 6       0.967641        0.999005    0.9960  0.990385  0.978881\n",
      "Dígito 7       0.987354        0.997548    0.9965  0.978785  0.983051\n",
      "Dígito 8       0.974333        0.997119    0.9949  0.973333  0.973833\n",
      "Dígito 9       0.980178        0.996441    0.9948  0.968658  0.974384\n"
     ]
    }
   ],
   "source": [
    "metricas_resultado = calcular_metricas(cm)\n",
    "df_resultado = pd.DataFrame.from_dict(metricas_resultado, orient='index',\n",
    "                                      columns=[\"Sensibilidade\", \"Especificidade\", \"Acurácia\", \"Precisão\", \"F-score\"])\n",
    "\n",
    "print(df_resultado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
